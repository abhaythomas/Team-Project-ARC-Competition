{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":67357,"databundleVersionId":8951125,"sourceType":"competition"},{"sourceId":8640486,"sourceType":"datasetVersion","datasetId":5174658},{"sourceId":8622192,"sourceType":"datasetVersion","datasetId":5123959},{"sourceId":33551,"sourceType":"modelInstanceVersion","modelInstanceId":28083,"modelId":39106},{"sourceId":116425,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":97840,"modelId":121027},{"sourceId":79488,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":66780,"modelId":91102},{"sourceId":141737,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":120063,"modelId":143287}],"dockerImageVersionId":30733,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**THIS IS COPIED CODE ONLY MEANT AS A STARTING POINT**","metadata":{}},{"cell_type":"markdown","source":"# Llama 3 8B Inference on the ARC Dataset\n\nIn this notebook, we will demonstrate how to use a fine-tuned version of Llama 3 8B to solve ARC tasks. You can also experiment with the original version or other models compatible with Hugging Face’s infrastructure. We utilize Hugging Face libraries due to their extensive documentation and ease of use. However, feel free to adapt this notebook using different packages to suit your needs.\n\nNote that this is not intended to be a sample solution and will likely not solve any tasks in the hidden test set. It’s just meant as a starting point for you to quickly get started, submit something, and receive a score.\n\nIf you have any additional sections you’d like to enhance or specific details to include, let us know!","metadata":{}},{"cell_type":"markdown","source":"## 1. Add Datasets and Model\n\nWe will be using the following datasets:\n\n1. ‘Abstraction and Reasoning Challenge’: This is the official dataset containing the ARC tasks to be solved.\n2. ‘Llama-3-8b-chat-ARC’: This dataset contains adapters for our Llama 3 model fine-tuned on the ARC dataset. !missing! (not public)\n3. (Optional) ‘Llama-3-ARC-deps’: This dataset contains the wheel files for additional packages not available in the Kaggle Kernel. Note that this dataset is required if you plan to submit this notebook to the competition, as no internet access is allowed during the competition. You can find it [here](https://www.kaggle.com/datasets/hansuelijud/llama-3-arc-deps).\n\nAdditionally, we need to add the original Llama 3 8B model:\n\n1. ‘Llama 3 8B-chat-hf’ (framework: transformers, variation: 8b-chat-hf, version: V1)\n\nPlease note that to access the Llama 3 model on Kaggle, you need to obtain access from Meta. Instructions on how to do this can be found [here](https://www.kaggle.com/models/metaresearch/llama-3).","metadata":{}},{"cell_type":"markdown","source":"## 2. Install and import Packages\n\nAs mentioned, we will be using Huggingface libraries, and most of the necessary packages are already available in Kaggle kernels. However, there are a few packages that are not included by default. If you are not submitting to the competition, you can download these packages directly.\n\nFor competition submissions, since internet access is restricted, we will use a Kaggle dataset containing the required wheel files. This allows us to install the packages without needing internet access during the submission process.","metadata":{}},{"cell_type":"markdown","source":"### 2.2 Without internet access (use for submission):\n\nIf we don't have internet access you can:\n1. Add the dataset we prepared [llama-3-arc-deps](https://www.kaggle.com/datasets/hansuelijud/llama-3-arc-deps).\n2. Create your own dataset. You can find the explanation [here](https://www.kaggle.com/code/hansuelijud/llama-3-8b-arc-additional-dependencies).","metadata":{}},{"cell_type":"markdown","source":"**Note: 2nd Option needs to studied further**","metadata":{}},{"cell_type":"code","source":"deps_path = '/kaggle/input/llama-3-arc-deps'\n! pip install --no-index --find-links {deps_path} --requirement {deps_path}/requirements.txt","metadata":{"execution":{"iopub.status.busy":"2024-10-23T08:41:33.154534Z","iopub.execute_input":"2024-10-23T08:41:33.154802Z","iopub.status.idle":"2024-10-23T08:41:51.276282Z","shell.execute_reply.started":"2024-10-23T08:41:33.154777Z","shell.execute_reply":"2024-10-23T08:41:51.275187Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Looking in links: /kaggle/input/llama-3-arc-deps\nProcessing /kaggle/input/llama-3-arc-deps/trl-0.9.3-py3-none-any.whl (from -r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1))\nProcessing /kaggle/input/llama-3-arc-deps/peft-0.11.1-py3-none-any.whl (from -r /kaggle/input/llama-3-arc-deps/requirements.txt (line 2))\nProcessing /kaggle/input/llama-3-arc-deps/bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (from -r /kaggle/input/llama-3-arc-deps/requirements.txt (line 4))\nRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (2.1.2)\nRequirement already satisfied: transformers>=4.31.0 in /opt/conda/lib/python3.10/site-packages (from trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (4.41.2)\nRequirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.10/site-packages (from trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (1.26.4)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (0.30.1)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (2.19.2)\nProcessing /kaggle/input/llama-3-arc-deps/tyro-0.8.4-py3-none-any.whl (from trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1))\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 2)) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 2)) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 2)) (6.0.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 2)) (4.66.4)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 2)) (0.4.3)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 2)) (0.23.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 2)) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 2)) (2024.3.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 2)) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 2)) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 2)) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (2023.12.25)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (0.19.1)\nRequirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (0.15)\nRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (13.7.0)\nProcessing /kaggle/input/llama-3-arc-deps/shtab-1.7.1-py3-none-any.whl (from tyro>=0.5.11->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1))\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (2.2.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (3.9.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (4.0.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 2)) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 2)) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 2)) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 2)) (2024.2.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (2.17.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (2023.4)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (0.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (1.16.0)\nInstalling collected packages: shtab, tyro, bitsandbytes, trl, peft\nSuccessfully installed bitsandbytes-0.43.1 peft-0.11.1 shtab-1.7.1 trl-0.9.3 tyro-0.8.4\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"### 2.3 Import Packages\n\nNow, let’s import the necessary packages:","metadata":{}},{"cell_type":"code","source":"# For dataset\nimport pandas as pd\nimport json\nimport os\nimport ast\nimport re\nimport numpy as np\nfrom datasets import Dataset\n\n# For LLM\nfrom peft import LoraConfig, PeftModel\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    set_seed,\n    pipeline\n)\nfrom trl import setup_chat_format\n\nimport torch\nfrom time import time\n\n# Set seed\nset_seed(42)","metadata":{"execution":{"iopub.status.busy":"2024-10-23T08:41:51.278234Z","iopub.execute_input":"2024-10-23T08:41:51.278538Z","iopub.status.idle":"2024-10-23T08:42:12.045963Z","shell.execute_reply.started":"2024-10-23T08:41:51.278509Z","shell.execute_reply":"2024-10-23T08:42:12.045187Z"},"trusted":true},"outputs":[{"name":"stderr","text":"2024-10-23 08:42:01.300803: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-10-23 08:42:01.300912: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-10-23 08:42:01.440077: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## 3. Load the data\n\nNext, let’s load the ARC tasks. Note that we will split tasks containing more than one test input into separate tasks, as this makes the pipeline easier. In the end, we will combine them again to create a valid submission file.","metadata":{}},{"cell_type":"code","source":"# Function to split the tasks that have multiple test input/output pairs.\n# This makes the handling easier, we will combine it again at the end for the submission\ndef split_dictionary(data):\n    \"\"\"\n    Splits the tasks that have multiple test input/output pairs into separate entries.\n\n    Args:\n    data (dict): The original dictionary containing tasks with 'test' and 'train' fields.\n\n    Returns:\n    tuple: A tuple containing:\n        - result (dict): The dictionary with tasks split into separate entries if they have multiple test pairs.\n        - split_files (list): A list of keys for the tasks that were split.\n    \"\"\"\n    result = {}\n    split_files = []\n    for key, value in data.items():\n        test_list = value.get(\"test\", [])\n        train_list = value.get(\"train\", [])\n        if len(test_list) > 1:\n            for idx, test_item in enumerate(test_list):\n                new_key = f\"{key}_{idx}\"\n                result[new_key] = {\n                    \"test\": [test_item],\n                    \"train\": train_list\n                }\n                split_files.append(new_key)\n        else:\n            result[key] = value\n    return result, split_files","metadata":{"execution":{"iopub.status.busy":"2024-10-23T08:42:12.047076Z","iopub.execute_input":"2024-10-23T08:42:12.047652Z","iopub.status.idle":"2024-10-23T08:42:12.054876Z","shell.execute_reply.started":"2024-10-23T08:42:12.047618Z","shell.execute_reply":"2024-10-23T08:42:12.054009Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Set test_run variable: False: create submission file for private test set, True: Evaluate on public tasks\ntest_run = False\n\n# Prepare data for DataFrame\n\n# Load JSON data from the files\nif test_run:\n    with open('/kaggle/input/arc-prize-2024/arc-agi_evaluation_challenges.json') as f:\n        challenges = json.load(f)\n        # Split tasks with multiple test inputs\n        challenges, split_files = split_dictionary(challenges) \n\n    with open('/kaggle/input/arc-prize-2024/arc-agi_evaluation_solutions.json') as f:\n        solutions = json.load(f)\nelse:\n    with open('/kaggle/input/arc-prize-2024/arc-agi_test_challenges.json') as f:\n        challenges = json.load(f)\n    # Split tasks with multiple test inputs\n    challenges, split_files = split_dictionary(challenges) \n\n# Print how many files have been split and their names\nsplit_file_count = len(split_files)//2\n\nprint(f\"Number of files split: {split_file_count}\")\nprint(\"File names:\")\nfor name in split_files:\n    print(name)\n\n# Prepare data\ndata = []\n        \nfor file_name, grids in challenges.items():\n    train_grids = grids.get('train', [])\n    test_inputs = grids.get('test', [])\n    if test_run:\n        # Handle files with multiple test inputs\n        parts = file_name.split('_')\n        if len(parts) > 1:\n            test_nr = int(parts[1])\n        else:\n            test_nr = 0\n        test_outputs = solutions.get(parts[0], [])\n        # Transform test grids to lists of dicts with 'output' key\n        test_outputs_transformed = [{'output': test_outputs[test_nr]}]\n        # Combine test inputs and outputs in alternating manner\n        combined_tests = [{'input': test_inputs[0]['input'], 'output': test_outputs_transformed[0]['output']}]\n    data.append({\n            'file_name': file_name,\n            'train': train_grids,\n            'test_input': test_inputs,\n            'test_output': test_outputs_transformed if test_run else [[0, 0]],\n            'test': combined_tests if test_run else test_inputs\n    })\n\n# Create DataFrame\ndf = pd.DataFrame(data)\n\n# Display the DataFrame\nprint(df)","metadata":{"execution":{"iopub.status.busy":"2024-10-23T08:42:12.056815Z","iopub.execute_input":"2024-10-23T08:42:12.057072Z","iopub.status.idle":"2024-10-23T08:42:12.178157Z","shell.execute_reply.started":"2024-10-23T08:42:12.057049Z","shell.execute_reply":"2024-10-23T08:42:12.177193Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Number of files split: 4\nFile names:\n239be575_0\n239be575_1\n25ff71a9_0\n25ff71a9_1\n27a28665_0\n27a28665_1\n27a28665_2\n3428a4f5_0\n3428a4f5_1\n    file_name                                              train  \\\n0    007bbfb7  [{'input': [[0, 7, 7], [7, 7, 7], [0, 7, 7]], ...   \n1    00d62c1b  [{'input': [[0, 0, 0, 0, 0, 0], [0, 0, 3, 0, 0...   \n2    017c7c7b  [{'input': [[0, 1, 0], [1, 1, 0], [0, 1, 0], [...   \n3    025d127b  [{'input': [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 6...   \n4    045e512c  [{'input': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n..        ...                                                ...   \n100  4290ef0e  [{'input': [[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...   \n101  42a50994  [{'input': [[0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0],...   \n102  4347f46a  [{'input': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n103  444801d8  [{'input': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0...   \n104  445eab21  [{'input': [[0, 7, 7, 7, 7, 0, 0, 0, 0, 0], [0...   \n\n                                            test_input test_output  \\\n0       [{'input': [[7, 0, 7], [7, 0, 7], [7, 7, 0]]}]    [[0, 0]]   \n1    [{'input': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    [[0, 0]]   \n2    [{'input': [[1, 1, 1], [0, 1, 0], [0, 1, 0], [...    [[0, 0]]   \n3    [{'input': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0...    [[0, 0]]   \n4    [{'input': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    [[0, 0]]   \n..                                                 ...         ...   \n100  [{'input': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...    [[0, 0]]   \n101  [{'input': [[0, 3, 0, 0, 3, 0, 0, 0, 0, 0, 3, ...    [[0, 0]]   \n102  [{'input': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    [[0, 0]]   \n103  [{'input': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1...    [[0, 0]]   \n104  [{'input': [[3, 3, 3, 3, 3, 0, 9, 9, 9, 9], [3...    [[0, 0]]   \n\n                                                  test  \n0       [{'input': [[7, 0, 7], [7, 0, 7], [7, 7, 0]]}]  \n1    [{'input': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n2    [{'input': [[1, 1, 1], [0, 1, 0], [0, 1, 0], [...  \n3    [{'input': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0...  \n4    [{'input': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n..                                                 ...  \n100  [{'input': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n101  [{'input': [[0, 3, 0, 0, 3, 0, 0, 0, 0, 0, 3, ...  \n102  [{'input': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n103  [{'input': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1...  \n104  [{'input': [[3, 3, 3, 3, 3, 0, 9, 9, 9, 9], [3...  \n\n[105 rows x 5 columns]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## 4. Load finetuned Llama-3 Model\n\nNext, we will load our fine-tuned Llama 3 model. We are using a 4-bit quantized version to reduce memory requirements. Ensure that you have selected an appropriate accelerator (e.g., P100) for the session to enable smooth functioning.","metadata":{}},{"cell_type":"code","source":"# Define a template for formatting chat messages with the Llama 3 model\n# This is model specific. Change it if you e.g. use Google's Gemma instead of Llama\nLLAMA_3_CHAT_TEMPLATE = \"\"\"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\"\"\"\n\n# Set the data type for computations to float16, bfloat16 not supported on T4/P100\ncompute_dtype = getattr(torch, \"float16\")\n\n# Configure the BitsAndBytes settings for 4-bit quantization to reduce memory usage\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,  # Enable 4-bit quantization\n    bnb_4bit_use_double_quant=True,  # Use double quantization for improved precision\n    bnb_4bit_quant_type=\"nf4\",  # Specify the quantization type\n    bnb_4bit_compute_dtype=compute_dtype,  # Set the computation data type\n)\n\n# Specify the model ID for loading the fine-tuned Llama 3 model\n# You can also test other models by replacing this line.\n# For the original non-finetuned model use\n#model_id = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"\n\n#model_id = \"/kaggle/input/llama-3.1/pytorch/8b-instruct/1\"\n\n#model_id = \"/kaggle/input/llama-3-8b-chat-hf-arc-finetune/\"\n\n# Record the start time to measure the loading duratio\ntime_start = time()\nprint(\"Loading model\")\n# Load the pre-trained model with specified configurations\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    trust_remote_code=True, # Allow the model to use custom code from the repository\n    quantization_config=bnb_config, # Apply the 4-bit quantization configuration\n    attn_implementation='sdpa', # Use scaled-dot product attention for better performance\n    torch_dtype=compute_dtype, # Set the data type for the model\n    use_cache=False, # Disable caching to save memory\n    device_map='auto', # Automatically map the model to available devices (e.g., GPUs)\n)\n\n# Load the tokenizer associated with the model\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.chat_template = LLAMA_3_CHAT_TEMPLATE # Apply the chat message template\n\n# Record the end time and print the duration for preparing the model and tokenizer\ntime_end = time()\nprint(f\"Prepare model, tokenizer: {round(time_end-time_start, 3)} sec.\")","metadata":{"execution":{"iopub.status.busy":"2024-10-23T08:42:12.179447Z","iopub.execute_input":"2024-10-23T08:42:12.179776Z","iopub.status.idle":"2024-10-23T08:42:13.248959Z","shell.execute_reply.started":"2024-10-23T08:42:12.179733Z","shell.execute_reply":"2024-10-23T08:42:13.247477Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Loading model\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Load the pre-trained model with specified configurations\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Allow the model to use custom code from the repository\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Apply the 4-bit quantization configuration\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msdpa\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Use scaled-dot product attention for better performance\u001b[39;49;00m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Set the data type for the model\u001b[39;49;00m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Disable caching to save memory\u001b[39;49;00m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Automatically map the model to available devices (e.g., GPUs)\u001b[39;49;00m\n\u001b[1;32m     35\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Load the tokenizer associated with the model\u001b[39;00m\n\u001b[1;32m     38\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:523\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    521\u001b[0m     _ \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 523\u001b[0m config, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs_orig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:934\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    931\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    932\u001b[0m code_revision \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 934\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    935\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    936\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/configuration_utils.py:632\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 632\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m    634\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/configuration_utils.py:689\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    685\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 689\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:370\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(resolved_file):\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _raise_exceptions_for_missing_entries:\n\u001b[0;32m--> 370\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    371\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Checkout \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/tree/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    373\u001b[0m         )\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    375\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","\u001b[0;31mOSError\u001b[0m: /kaggle/input/llama-3.1/pytorch/8b-instruct/1 does not appear to have a file named config.json. Checkout 'https://huggingface.co//kaggle/input/llama-3.1/pytorch/8b-instruct/1/tree/None' for available files."],"ename":"OSError","evalue":"/kaggle/input/llama-3.1/pytorch/8b-instruct/1 does not appear to have a file named config.json. Checkout 'https://huggingface.co//kaggle/input/llama-3.1/pytorch/8b-instruct/1/tree/None' for available files.","output_type":"error"}],"execution_count":5},{"cell_type":"markdown","source":"## 5. Create Prompts and filter the dataset\n\nNext, we will create the prompts that will be used to evaluate the model on the ARC dataset.","metadata":{}},{"cell_type":"markdown","source":"### 5.1 Create Prompts\n","metadata":{}},{"cell_type":"code","source":"# The system_prompt defines the initial instructions for the model, setting the context for solving ARC tasks.\nsystem_prompt = '''You are a puzzle solving wizard. You are given a puzzle from the abstraction and reasoning corpus developed by Francois Chollet.'''\n\n# User message template is a template for creating user prompts. It includes placeholders for training data and test input data, guiding the model to learn the rule and apply it to solve the given puzzle.\nuser_message_template = '''Here are the example input and output pairs from which you should learn the underlying rule to later predict the output for the given test input:\n----------------------------------------\n{training_data}\n----------------------------------------\nNow, solve the following puzzle based on its input grid by applying the rules you have learned from the training data.:\n----------------------------------------\n[{{'input': {input_test_data}, 'output': [[]]}}]\n----------------------------------------\nWhat is the output grid? Only provide the output grid in the form as in the example input and output pairs. Do not provide any additional information:'''\n\ndef preprocess(task, test_run, train_mode=False):\n    \"\"\"\n    Preprocess a single ARC task to create the prompt and solution for the model.\n\n    This function formats the system and user messages using a predefined template and the task's training and test data.\n    If in training mode, it also includes the assistant's message with the expected output.\n\n    Parameters:\n    task (dict): The ARC task data containing training and test examples.\n    train_mode (bool): If True, includes the assistant's message with the expected output for training purposes.\n\n    Returns:\n    dict: A dictionary containing the formatted text prompt, the solution, and the file name.\n    \"\"\"\n    # System message\n    system_message = {\"role\": \"system\", \"content\": system_prompt}\n\n    # Extract training data and input grid from the task\n    training_data = task['train']\n    input_test_data = task['test'][0]['input']\n    if test_run:\n        output_test_data = task['test'][0]['output']\n    else:\n        output_test_data = [[0 ,0]]\n\n    # Format the user message with training data and input test data\n    user_message_content = user_message_template.format(training_data=training_data, input_test_data=input_test_data)\n    user_message = {\n        \"role\": \"user\",\n        \"content\": user_message_content\n    }\n\n    # Include the assistant message with the expected output if in training mode\n    if train_mode:\n        assistant_message = {\n            \"role\": \"assistant\",\n            \"content\": str(output_test_data)\n        }\n\n        # Combine system, user, and assistant messages\n        messages = [system_message, user_message, assistant_message]\n    else:\n        messages = [system_message, user_message]\n    # Convert messages using the chat template for use with the instruction finetuned version of Llama\n    messages = tokenizer.apply_chat_template(messages, tokenize=False)\n    if test_run:\n        return {\"text\": messages, \"solution\": output_test_data, \"file_name\": task['file_name']}\n    else:\n        return {\"text\": messages, \"file_name\": task['file_name']}\n\n# Convert the loaded data to a Huggingface Dataset object\ndataset = Dataset.from_pandas(df)\n\n# Apply the preprocess function to each task in the dataset\ndataset = dataset.map(lambda x: preprocess(x, test_run), batched=False, remove_columns=dataset.column_names)","metadata":{"execution":{"iopub.status.busy":"2024-10-23T08:42:13.249885Z","iopub.status.idle":"2024-10-23T08:42:13.250212Z","shell.execute_reply.started":"2024-10-23T08:42:13.250044Z","shell.execute_reply":"2024-10-23T08:42:13.250057Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5.2 Filter the Dataset\n\nTo ensure that the model’s context window is not exceeded, we will restrict the tasks to those with prompts below a specified maximum number of tokens. Due to memory limitations, the model has been fine-tuned only on tasks with up to 2048 tokens.","metadata":{}},{"cell_type":"code","source":"# Define the maximum number of tokens allowed\nmax_tokens = 8000  # Adjust this value as needed\n\n\n# Function to calculate the number of tokens\ndef count_tokens(text):\n    \"\"\"\n    Calculate the number of tokens in a given text using the tokenizer.\n\n    This function uses the tokenizer to encode the input text and returns the\n    number of tokens. It is useful for ensuring that the text length stays\n    within the model's context window.\n\n    Parameters:\n    text (str): The input text to be tokenized.\n\n    Returns:\n    int: The number of tokens in the input text.\n    \"\"\"\n    return len(tokenizer.encode(text))\n\n# Filter the dataset to include only tasks with a number of tokens within the allowed limit\nfiltered_dataset = dataset.filter(lambda x: count_tokens(x['text']) <= max_tokens)\n\n# Print the number of tasks filtered out and the remaining tasks\nprint(f'{len(dataset)-len(filtered_dataset)} tasks contain too many tokens if we set max_tokens to {max_tokens}')\nprint(f'The dataset contains {len(filtered_dataset)} tasks to evaluate the model')","metadata":{"execution":{"iopub.status.busy":"2024-10-23T08:42:13.251481Z","iopub.status.idle":"2024-10-23T08:42:13.251812Z","shell.execute_reply.started":"2024-10-23T08:42:13.251651Z","shell.execute_reply":"2024-10-23T08:42:13.251665Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Evaluate the model\n\nNow, let’s query the Language Learning Model (LLM) to determine its ability to successfully solve ARC tasks:","metadata":{}},{"cell_type":"markdown","source":"### 6.1 Generate the Outputs\n","metadata":{}},{"cell_type":"code","source":"# Define your LLM pipeline\ntext_gen_pipeline = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\n\n# Define terminators for the pipeline\nterminators = [\n    text_gen_pipeline.tokenizer.eos_token_id,\n    text_gen_pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n]\n\n# Function to generate outputs\ndef generate_solution(task, max_new_tokens=512, do_sample=True, temperature=0.1, top_p=0.1):\n    \"\"\"\n    Generate a solution for an ARC task using the language model.\n\n    This function takes a task prompt, generates a solution using the text generation pipeline,\n    and extracts the generated solution from the model's output.\n\n    Parameters:\n    task (dict): The ARC task data containing the prompt and other relevant information.\n    max_new_tokens (int, optional): The maximum number of new tokens to generate. Default is 512.\n    do_sample (bool, optional): Whether to use sampling; if False, greedy decoding is used. Default is True.\n    temperature (float, optional): The sampling temperature. Lower values make the model more conservative. Default is 0.1.\n    top_p (float, optional): The cumulative probability for nucleus sampling. Lower values make the model more conservative. Default is 0.1.\n\n    Returns:\n    dict: A dictionary containing the generated solution.\n    \"\"\"\n    # Extract the prompt from the task\n    prompt = task['text']\n    \n    # Generate the model's output based on the prompt\n    outputs = text_gen_pipeline(\n        prompt, \n        max_new_tokens=max_new_tokens, \n        eos_token_id=terminators, \n        do_sample=do_sample, \n        temperature=temperature, \n        top_p=top_p\n    )\n    \n    # Extract the generated solution from the model's output\n    generated_solutions = outputs[0][\"generated_text\"][len(prompt):]\n    return {'generated_solution': generated_solutions}\n\n# Generate solutions\nprint(\"Generating solutions\")\nfiltered_dataset = filtered_dataset.map(generate_solution, batched=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-23T08:42:13.253469Z","iopub.status.idle":"2024-10-23T08:42:13.253914Z","shell.execute_reply.started":"2024-10-23T08:42:13.253681Z","shell.execute_reply":"2024-10-23T08:42:13.253699Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(filtered_dataset[:5]['generated_solution'])","metadata":{"execution":{"iopub.status.busy":"2024-10-23T08:42:13.255493Z","iopub.status.idle":"2024-10-23T08:42:13.255822Z","shell.execute_reply.started":"2024-10-23T08:42:13.255661Z","shell.execute_reply":"2024-10-23T08:42:13.255675Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 6.2 Extract Solutions and Verify Validity\n\nNext, we will extract the generated solutions from the model’s output and verify their validity.","metadata":{}},{"cell_type":"code","source":"def extract_solution(text):\n    \"\"\"\n    Extract the solution array from the generated text.\n\n    Parameters:\n    text (str): The text containing the generated solution.\n\n    Returns:\n    list: A list of lists representing the extracted solution array.\n          Returns [[0]] if no valid solution is found.\n    \"\"\"\n    try:\n        # Find the part of the text that looks like a nested list\n        start = text.index('[[')\n        end = text.index(']]', start) + 2\n        array_str = text[start:end]\n        \n        # Use ast.literal_eval to safely evaluate the string as a Python expression\n        array = ast.literal_eval(array_str)\n        \n        # Check if the result is a list of lists\n        if all(isinstance(i, list) for i in array):\n            return array\n        else:\n            return [[0]]\n    except (ValueError, SyntaxError):\n        return [[0]]\n\ndef pad_array_with_value(array, target_shape, pad_value):\n    \"\"\"\n    Pad the given array to the target shape with the specified pad value.\n\n    This function pads the original array to fit the target shape by adding additional\n    pixels at the ends. This method ensures that the smaller array is placed at the\n    top-left corner of the target shape, making sense of the number of correct pixels\n    during comparison.\n\n    Note:\n    Depending on how you pad the arrays, the number of correct pixels might vary.\n    For example, placing the smaller array in the center versus adding pixels at the ends\n    can yield different results. Here, we pad by adding pixels at the ends.\n\n    Parameters:\n    array (list): The original array to be padded.\n    target_shape (tuple): The desired shape of the padded array (rows, columns).\n    pad_value (int): The value to use for padding the array.\n\n    Returns:\n    np.ndarray: A padded array with the specified target shape and pad value.\n    \"\"\"\n    padded_array = np.full(target_shape, pad_value, dtype=int)\n    original_shape = np.array(array).shape\n    padded_array[:original_shape[0], :original_shape[1]] = array\n    return padded_array\n\ndef compare_solutions_with_padding(generated_output, correct_output, pad_value=-1):\n    \"\"\"\n    Compare the generated output with the correct output, using padding to align their shapes.\n\n    Parameters:\n    generated_output (list): The generated solution array.\n    correct_output (list): The correct solution array.\n    pad_value (int, optional): The value to use for padding. Default is -1. The colour value -1 should not be present in the solutions.\n\n    Returns:\n    tuple: A tuple containing:\n        - is_correct (bool): True if the solutions match exactly, False otherwise.\n        - correct_percentage (float): The percentage of correctly matched pixels.\n    \"\"\"\n    max_rows = max(len(generated_output), len(correct_output))\n    max_cols = max(len(generated_output[0]), len(correct_output[0]))\n    target_shape = (max_rows, max_cols)\n    \n    padded_generated = pad_array_with_value(generated_output, target_shape, pad_value)\n    padded_correct = pad_array_with_value(correct_output, target_shape, pad_value)\n    \n    total_pixels = max_rows * max_cols\n    correct_pixels = np.sum((padded_generated == padded_correct) & (padded_generated != pad_value) & (padded_correct != pad_value))\n    correct_percentage = (correct_pixels / total_pixels) * 100\n    \n    is_correct = (correct_pixels == total_pixels)\n    \n    return is_correct, correct_percentage\n\nif test_run:\n    # Lists to store results of task evaluation\n    solved_tasks = []\n    failed_tasks = []\n    accuracy_list = []\n\n    for i, task in enumerate(filtered_dataset):\n        true_solution = task['solution']\n        file_name = task['file_name']\n        generated_text = task[\"generated_solution\"]\n\n        # Extract the solution generated by the model\n        gen_solution = extract_solution(generated_text)\n\n        # Compare the generated solution with the true solution\n        is_correct, correct_percentage = compare_solutions_with_padding(gen_solution, true_solution)\n\n        # Append results to respective lists based on correctness\n        if is_correct:\n            solved_tasks.append({\n                'file_name': file_name,\n                'llm_output': generated_text,\n                'solution': gen_solution\n            })\n        else:\n            failed_tasks.append({\n                'file_name': file_name,\n                'llm_output': generated_text,\n                'solution': gen_solution\n            })\n\n        # Store \"pixel accuracy for each task\n        accuracy_list.append({\n            'file_name': file_name,\n            'correct_percentage': correct_percentage\n        })\n\n    # Create a dictionary to store results\n    results = {'file_name': [], 'solved': [], 'accuracy': []}\n\n    # Add solved tasks to the results\n    for task in solved_tasks:\n        results['file_name'].append(task['file_name'])\n        results['solved'].append(True)\n        results['accuracy'].append(next((item['correct_percentage'] for item in accuracy_list if item['file_name'] == task['file_name']), None))\n\n    # Add failed tasks to the results\n    for task in failed_tasks:\n        results['file_name'].append(task['file_name'])\n        results['solved'].append(False)\n        results['accuracy'].append(next((item['correct_percentage'] for item in accuracy_list if item['file_name'] == task['file_name']), None))\n\n    # Create a DataFrame\n    df_results = pd.DataFrame(results)\n\n    # Display the DataFrame as a table\n    print(df_results)\n\n    # Calculate and print the average correct percentage\n    average_correct_percentage = df_results['accuracy'].mean()\n    print(f\"Average 'Pixel Accuracy' of attempted tasks: {average_correct_percentage:.2f}%\")\n\n    # Calculate and print the number of solved tasks out of the total number of tasks\n    total_tasks = len(df)\n    solved_tasks_count = df_results['solved'].sum()\n    print(f\"Solved {solved_tasks_count} out of {total_tasks} tasks ({(solved_tasks_count / total_tasks) * 100:.2f}%)\")","metadata":{"execution":{"iopub.status.busy":"2024-10-23T08:42:13.257028Z","iopub.status.idle":"2024-10-23T08:42:13.257370Z","shell.execute_reply.started":"2024-10-23T08:42:13.257208Z","shell.execute_reply":"2024-10-23T08:42:13.257223Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Submit\n\nFor submission you must create a file called 'submission.json' which should have the format as explained [here](www.kaggle.com/competitions/arc-prize-2024/overview/evaluation)\n\n`\n{\"00576224\": [{\"attempt_1\": [[0, 0], [0, 0]], \"attempt_2\": [[0, 0], [0, 0]]}],\n \"009d5c81\": [{\"attempt_1\": [[0, 0], [0, 0]], \"attempt_2\": [[0, 0], [0, 0]]}],\n \"12997ef3\": [{\"attempt_1\": [[0, 0], [0, 0]], \"attempt_2\": [[0, 0], [0, 0]]},\n              {\"attempt_1\": [[0, 0], [0, 0]], \"attempt_2\": [[0, 0], [0, 0]]}],\n ...\n}\n`","metadata":{}},{"cell_type":"code","source":"solution_dict = {}\n\nfor i, task in enumerate(filtered_dataset):\n    file_name = task['file_name']\n    generated_text = task[\"generated_solution\"]\n    # Extract the solution generated by the model\n    gen_solution = extract_solution(generated_text)\n    # For now we only do one attempt\n    solution_dict[file_name] = [\n        {\n            \"attempt_1\": gen_solution,\n            \"attempt_2\": [[0, 0], [0, 0]]\n        }\n    ]\n\n# Recombining the solutions for split files\ncombined_solution_dict = {}\ncombined_files = {}\n\nfor file_name, attempts in solution_dict.items():\n    base_name = file_name.split('_')[0]\n    if base_name not in combined_solution_dict:\n        combined_solution_dict[base_name] = []\n        combined_files[base_name] = []\n    combined_solution_dict[base_name].extend(attempts)\n    if '_' in file_name:\n        combined_files[base_name].append(file_name)\n        \n# Printing which file names have been combined\nprint(\"Files that have been combined:\")\nfor base_name, files in combined_files.items():\n    if files:  # Print only if there are files that were combined\n        print(f\"{base_name}: {', '.join(files)}\")\n\n# We still need to fill in dummy solutions for the tasks we did not consider to make a valid submission:\n# Load the sample submission file\nwith open('/kaggle/input/arc-prize-2024/sample_submission.json') as f:\n    sample_submission = json.load(f)\n# Fill in all entries that are still missing from the sample_submission file\nfor key, value in sample_submission.items():\n    if key not in combined_solution_dict:\n        combined_solution_dict[key] = value\n\n# Create submission\nwith open(\"submission.json\", \"w\") as json_file:\n    json.dump(combined_solution_dict, json_file) ","metadata":{"execution":{"iopub.status.busy":"2024-10-23T08:42:13.258987Z","iopub.status.idle":"2024-10-23T08:42:13.259329Z","shell.execute_reply.started":"2024-10-23T08:42:13.259159Z","shell.execute_reply":"2024-10-23T08:42:13.259180Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Closing Remarks\n\nThe fine-tuned Llama 3 model used in this notebook has not been optimized and will likely not successfully solve any of the hidden test tasks. The primary purpose of this fine-tuning is to ensure that the model’s responses are specific to the task, making it easier to extract the solution array. It is unlikely that the model has significantly improved in solving ARC tasks through this fine-tuning process. It is your task to change that ;).\n\nIt’s worth noting that Large Language Model (LLM) performance might improve with techniques like chain-of-thought prompting, where the model is asked to explain its reasoning process. While this approach can enhance understanding, it may also complicate the extraction of the solution array and take more time. You can find an overview of recent papers and repositories [here](https://docs.google.com/spreadsheets/d/1fR4cgjY1kNKN_dxiidBQbyT6Gv7_Ko7daKOjlYojwTY/edit#gid=756763742).\n\nFor further insights and state-of-the-art performance on ARC using a fine-tuned LLM, you can refer to the work of Jack Cole and Mohamed Osman. The notebook they submitted to the past challenge can be found [here](https://www.kaggle.com/code/jcole75/mindsai-nlp-arc) and a recent interview on the [lab42.global](https://lab42.global/community-2023-july-arc-sota/) webpage. His approach operates on a significantly different scale and showcases advanced techniques and optimizations.\n\nFor more inspiration head over to the official [arcprize.org](https://arcprize.org/) webpage and have a lookt at the community [resources](https://arcprize.org/guide#resources) list.","metadata":{}}]}