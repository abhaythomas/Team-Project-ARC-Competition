{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T06:18:15.711551Z",
     "iopub.status.busy": "2024-06-12T06:18:15.71122Z",
     "iopub.status.idle": "2024-06-12T06:18:15.715434Z",
     "shell.execute_reply": "2024-06-12T06:18:15.714591Z",
     "shell.execute_reply.started": "2024-06-12T06:18:15.711525Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install -q -U -i https://pypi.org/simple/ bitsandbytes\n",
    "# !pip install -q -U trl\n",
    "# !pip install -q -U peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T06:07:40.513166Z",
     "iopub.status.busy": "2024-06-12T06:07:40.512821Z",
     "iopub.status.idle": "2024-06-12T06:07:58.400491Z",
     "shell.execute_reply": "2024-06-12T06:07:58.399342Z",
     "shell.execute_reply.started": "2024-06-12T06:07:40.513137Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "deps_path = '/kaggle/input/llama-3-arc-deps'\n",
    "! pip install --no-index --find-links {deps_path} --requirement {deps_path}/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T06:08:01.303004Z",
     "iopub.status.busy": "2024-06-12T06:08:01.302631Z",
     "iopub.status.idle": "2024-06-12T06:08:19.630559Z",
     "shell.execute_reply": "2024-06-12T06:08:19.629592Z",
     "shell.execute_reply.started": "2024-06-12T06:08:01.302964Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# For dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import ast\n",
    "import re\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "# For LLM\n",
    "from peft import LoraConfig, PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    set_seed,\n",
    "    pipeline\n",
    ")\n",
    "from trl import setup_chat_format\n",
    "\n",
    "import torch\n",
    "from time import time\n",
    "\n",
    "# Set seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T06:08:26.76681Z",
     "iopub.status.busy": "2024-06-12T06:08:26.765508Z",
     "iopub.status.idle": "2024-06-12T06:08:26.781335Z",
     "shell.execute_reply": "2024-06-12T06:08:26.78023Z",
     "shell.execute_reply.started": "2024-06-12T06:08:26.766719Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to split the tasks that have multiple test input/output pairs.\n",
    "# This makes the handling easier, we will combine it again at the end for the submission\n",
    "def split_dictionary(data):\n",
    "    \n",
    "    result = {}\n",
    "    split_files = []\n",
    "    for key, value in data.items():\n",
    "        test_list = value.get(\"test\", [])\n",
    "        train_list = value.get(\"train\", [])\n",
    "        if len(test_list) > 1:\n",
    "            for idx, test_item in enumerate(test_list):\n",
    "                new_key = f\"{key}_{idx}\"\n",
    "                result[new_key] = {\n",
    "                    \"test\": [test_item],\n",
    "                    \"train\": train_list\n",
    "                }\n",
    "                split_files.append(new_key)\n",
    "        else:\n",
    "            result[key] = value\n",
    "    return result, split_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T06:08:28.820797Z",
     "iopub.status.busy": "2024-06-12T06:08:28.820126Z",
     "iopub.status.idle": "2024-06-12T06:08:28.913481Z",
     "shell.execute_reply": "2024-06-12T06:08:28.912448Z",
     "shell.execute_reply.started": "2024-06-12T06:08:28.820763Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set test_run variable: False: create submission file for private test set, True: Evaluate on public tasks\n",
    "test_run = False\n",
    "\n",
    "# Prepare data for DataFrame\n",
    "\n",
    "# Load JSON data from the files\n",
    "if test_run:\n",
    "    with open('/kaggle/input/arc-prize-2024/arc-agi_evaluation_challenges.json') as f:\n",
    "        challenges = json.load(f)\n",
    "        # Split tasks with multiple test inputs\n",
    "        challenges, split_files = split_dictionary(challenges) \n",
    "\n",
    "    with open('/kaggle/input/arc-prize-2024/arc-agi_evaluation_solutions.json') as f:\n",
    "        solutions = json.load(f)\n",
    "else:\n",
    "    with open('/kaggle/input/arc-prize-2024/arc-agi_test_challenges.json') as f:\n",
    "        challenges = json.load(f)\n",
    "    # Split tasks with multiple test inputs\n",
    "    challenges, split_files = split_dictionary(challenges) \n",
    "\n",
    "# Print how many files have been split and their names\n",
    "split_file_count = len(split_files)//2\n",
    "\n",
    "print(f\"Number of files split: {split_file_count}\")\n",
    "print(\"File names:\")\n",
    "for name in split_files:\n",
    "    print(name)\n",
    "\n",
    "# Prepare data\n",
    "data = []\n",
    "        \n",
    "for file_name, grids in challenges.items():\n",
    "    train_grids = grids.get('train', [])\n",
    "    test_inputs = grids.get('test', [])\n",
    "    if test_run:\n",
    "        # Handle files with multiple test inputs\n",
    "        parts = file_name.split('_')\n",
    "        if len(parts) > 1:\n",
    "            test_nr = int(parts[1])\n",
    "        else:\n",
    "            test_nr = 0\n",
    "        test_outputs = solutions.get(parts[0], [])\n",
    "        # Transform test grids to lists of dicts with 'output' key\n",
    "        test_outputs_transformed = [{'output': test_outputs[test_nr]}]\n",
    "        # Combine test inputs and outputs in alternating manner\n",
    "        combined_tests = [{'input': test_inputs[0]['input'], 'output': test_outputs_transformed[0]['output']}]\n",
    "    data.append({\n",
    "            'file_name': file_name,\n",
    "            'train': train_grids,\n",
    "            'test_input': test_inputs,\n",
    "            'test_output': test_outputs_transformed if test_run else [[0, 0]],\n",
    "            'test': combined_tests if test_run else test_inputs\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T06:08:36.892437Z",
     "iopub.status.busy": "2024-06-12T06:08:36.891978Z",
     "iopub.status.idle": "2024-06-12T06:10:16.969322Z",
     "shell.execute_reply": "2024-06-12T06:10:16.968432Z",
     "shell.execute_reply.started": "2024-06-12T06:08:36.8924Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define a template for formatting chat messages with the Llama 3 model\n",
    "# This is model specific. Change it if you e.g. use Google's Gemma instead of Llama\n",
    "LLAMA_3_CHAT_TEMPLATE = \"\"\"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\"\"\"\n",
    "\n",
    "# Set the data type for computations to float16, bfloat16 not supported on T4/P100\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "# Configure the BitsAndBytes settings for 4-bit quantization to reduce memory usage\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Enable 4-bit quantization\n",
    "    bnb_4bit_use_double_quant=True,  # Use double quantization for improved precision\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Specify the quantization type\n",
    "    bnb_4bit_compute_dtype=compute_dtype,  # Set the computation data type\n",
    ")\n",
    "\n",
    "# Specify the model ID for loading the fine-tuned Llama 3 model\n",
    "# You can also test other models by replacing this line.\n",
    "# For the original non-finetuned model use\n",
    "# model_id = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"\n",
    "model_id = \"/kaggle/input/llama-3-8b-chat-hf-arc-finetune/\"\n",
    "\n",
    "# Record the start time to measure the loading duratio\n",
    "time_start = time()\n",
    "print(\"Loading model\")\n",
    "# Load the pre-trained model with specified configurations\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True, # Allow the model to use custom code from the repository\n",
    "    quantization_config=bnb_config, # Apply the 4-bit quantization configuration\n",
    "    attn_implementation='sdpa', # Use scaled-dot product attention for better performance\n",
    "    torch_dtype=compute_dtype, # Set the data type for the model\n",
    "    use_cache=False, # Disable caching to save memory\n",
    "    device_map='auto', # Automatically map the model to available devices (e.g., GPUs)\n",
    ")\n",
    "\n",
    "# Load the tokenizer associated with the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.chat_template = LLAMA_3_CHAT_TEMPLATE # Apply the chat message template\n",
    "\n",
    "# Record the end time and print the duration for preparing the model and tokenizer\n",
    "time_end = time()\n",
    "print(f\"Prepare model, tokenizer: {round(time_end-time_start, 3)} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T06:12:16.792871Z",
     "iopub.status.busy": "2024-06-12T06:12:16.792166Z",
     "iopub.status.idle": "2024-06-12T06:12:17.274618Z",
     "shell.execute_reply": "2024-06-12T06:12:17.273755Z",
     "shell.execute_reply.started": "2024-06-12T06:12:16.79284Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# The system_prompt defines the initial instructions for the model, setting the context for solving ARC tasks.\n",
    "system_prompt = '''You are a puzzle solving wizard. You are given a puzzle from the abstraction and reasoning corpus developed by Francois Chollet.'''\n",
    "\n",
    "# User message template is a template for creating user prompts. It includes placeholders for training data and test input data, guiding the model to learn the rule and apply it to solve the given puzzle.\n",
    "user_message_template = '''Here are the example input and output pairs from which you should learn the underlying rule to later predict the output for the given test input:\n",
    "----------------------------------------\n",
    "{training_data}\n",
    "----------------------------------------\n",
    "Now, solve the following puzzle based on its input grid by applying the rules you have learned from the training data.:\n",
    "----------------------------------------\n",
    "[{{'input': {input_test_data}, 'output': [[]]}}]\n",
    "----------------------------------------'''\n",
    "\n",
    "def preprocess(task, test_run, train_mode=False):\n",
    "    # System message\n",
    "    system_message = {\"role\": \"system\", \"content\": system_prompt}\n",
    "\n",
    "    # Extract training data and input grid from the task\n",
    "    training_data = task['train']\n",
    "    input_test_data = task['test'][0]['input']\n",
    "    if test_run:\n",
    "        output_test_data = task['test'][0]['output']\n",
    "    else:\n",
    "        output_test_data = [[0 ,0]]\n",
    "\n",
    "    # Format the user message with training data and input test data\n",
    "    user_message_content = user_message_template.format(training_data=training_data, input_test_data=input_test_data)\n",
    "    user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_message_content\n",
    "    }\n",
    "\n",
    "    # Include the assistant message with the expected output if in training mode\n",
    "    if train_mode:\n",
    "        assistant_message = {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": str(output_test_data)\n",
    "        }\n",
    "\n",
    "        # Combine system, user, and assistant messages\n",
    "        messages = [system_message, user_message, assistant_message]\n",
    "    else:\n",
    "        messages = [system_message, user_message]\n",
    "    # Convert messages using the chat template for use with the instruction finetuned version of Llama\n",
    "    messages = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    if test_run:\n",
    "        return {\"text\": messages, \"solution\": output_test_data, \"file_name\": task['file_name']}\n",
    "    else:\n",
    "        return {\"text\": messages, \"file_name\": task['file_name']}\n",
    "\n",
    "# Convert the loaded data to a Huggingface Dataset object\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Apply the preprocess function to each task in the dataset\n",
    "dataset = dataset.map(lambda x: preprocess(x, test_run), batched=False, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T06:17:11.727696Z",
     "iopub.status.busy": "2024-06-12T06:17:11.727194Z",
     "iopub.status.idle": "2024-06-12T06:17:12.642496Z",
     "shell.execute_reply": "2024-06-12T06:17:12.641465Z",
     "shell.execute_reply.started": "2024-06-12T06:17:11.727664Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the maximum number of tokens allowed\n",
    "max_tokens = 8000  # Adjust this value as needed\n",
    "\n",
    "\n",
    "# Function to calculate the number of tokens\n",
    "def count_tokens(text):\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "# Filter the dataset to include only tasks with a number of tokens within the allowed limit\n",
    "filtered_dataset = dataset.filter(lambda x: count_tokens(x['text']) <= max_tokens)\n",
    "\n",
    "# Print the number of tasks filtered out and the remaining tasks\n",
    "print(f'{len(dataset)-len(filtered_dataset)} tasks contain too many tokens if we set max_tokens to {max_tokens}')\n",
    "print(f'The dataset contains {len(filtered_dataset)} tasks to evaluate the model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T06:17:17.209832Z",
     "iopub.status.busy": "2024-06-12T06:17:17.209405Z",
     "iopub.status.idle": "2024-06-12T06:18:15.653896Z",
     "shell.execute_reply": "2024-06-12T06:18:15.653033Z",
     "shell.execute_reply.started": "2024-06-12T06:17:17.2098Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define your LLM pipeline\n",
    "text_gen_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Define terminators for the pipeline\n",
    "terminators = [\n",
    "    text_gen_pipeline.tokenizer.eos_token_id,\n",
    "    text_gen_pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "# Function to generate outputs\n",
    "def generate_solution(task, max_new_tokens=512, do_sample=True, temperature=0.1, top_p=0.1):\n",
    "\n",
    "    # Extract the prompt from the task\n",
    "    prompt = task['text']\n",
    "    \n",
    "    # Generate the model's output based on the prompt\n",
    "    outputs = text_gen_pipeline(\n",
    "        prompt, \n",
    "        max_new_tokens=max_new_tokens, \n",
    "        eos_token_id=terminators, \n",
    "        do_sample=do_sample, \n",
    "        temperature=temperature, \n",
    "        top_p=top_p\n",
    "    )\n",
    "    \n",
    "    # Extract the generated solution from the model's output\n",
    "    generated_solutions = outputs[0][\"generated_text\"][len(prompt):]\n",
    "    return {'generated_solution': generated_solutions}\n",
    "\n",
    "# Generate solutions\n",
    "print(\"Generating solutions\")\n",
    "filtered_dataset = filtered_dataset.map(generate_solution, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T06:18:15.655732Z",
     "iopub.status.busy": "2024-06-12T06:18:15.655461Z",
     "iopub.status.idle": "2024-06-12T06:18:15.660666Z",
     "shell.execute_reply": "2024-06-12T06:18:15.659847Z",
     "shell.execute_reply.started": "2024-06-12T06:18:15.655708Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(filtered_dataset[:5]['generated_solution'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T06:18:15.662332Z",
     "iopub.status.busy": "2024-06-12T06:18:15.662015Z",
     "iopub.status.idle": "2024-06-12T06:18:15.682678Z",
     "shell.execute_reply": "2024-06-12T06:18:15.681772Z",
     "shell.execute_reply.started": "2024-06-12T06:18:15.662282Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_solution(text):\n",
    "\n",
    "    try:\n",
    "        # Find the part of the text that looks like a nested list\n",
    "        start = text.index('[[')\n",
    "        end = text.index(']]', start) + 2\n",
    "        array_str = text[start:end]\n",
    "        \n",
    "        # Use ast.literal_eval to safely evaluate the string as a Python expression\n",
    "        array = ast.literal_eval(array_str)\n",
    "        \n",
    "        # Check if the result is a list of lists\n",
    "        if all(isinstance(i, list) for i in array):\n",
    "            return array\n",
    "        else:\n",
    "            return [[0]]\n",
    "    except (ValueError, SyntaxError):\n",
    "        return [[0]]\n",
    "\n",
    "def pad_array_with_value(array, target_shape, pad_value):\n",
    "    \n",
    "    padded_array = np.full(target_shape, pad_value, dtype=int)\n",
    "    original_shape = np.array(array).shape\n",
    "    padded_array[:original_shape[0], :original_shape[1]] = array\n",
    "    return padded_array\n",
    "\n",
    "def compare_solutions_with_padding(generated_output, correct_output, pad_value=-1):\n",
    "    max_rows = max(len(generated_output), len(correct_output))\n",
    "    max_cols = max(len(generated_output[0]), len(correct_output[0]))\n",
    "    target_shape = (max_rows, max_cols)\n",
    "    \n",
    "    padded_generated = pad_array_with_value(generated_output, target_shape, pad_value)\n",
    "    padded_correct = pad_array_with_value(correct_output, target_shape, pad_value)\n",
    "    \n",
    "    total_pixels = max_rows * max_cols\n",
    "    correct_pixels = np.sum((padded_generated == padded_correct) & (padded_generated != pad_value) & (padded_correct != pad_value))\n",
    "    correct_percentage = (correct_pixels / total_pixels) * 100\n",
    "    \n",
    "    is_correct = (correct_pixels == total_pixels)\n",
    "    \n",
    "    return is_correct, correct_percentage\n",
    "\n",
    "if test_run:\n",
    "    # Lists to store results of task evaluation\n",
    "    solved_tasks = []\n",
    "    failed_tasks = []\n",
    "    accuracy_list = []\n",
    "\n",
    "    for i, task in enumerate(filtered_dataset):\n",
    "        true_solution = task['solution']\n",
    "        file_name = task['file_name']\n",
    "        generated_text = task[\"generated_solution\"]\n",
    "\n",
    "        # Extract the solution generated by the model\n",
    "        gen_solution = extract_solution(generated_text)\n",
    "\n",
    "        # Compare the generated solution with the true solution\n",
    "        is_correct, correct_percentage = compare_solutions_with_padding(gen_solution, true_solution)\n",
    "\n",
    "        # Append results to respective lists based on correctness\n",
    "        if is_correct:\n",
    "            solved_tasks.append({\n",
    "                'file_name': file_name,\n",
    "                'llm_output': generated_text,\n",
    "                'solution': gen_solution\n",
    "            })\n",
    "        else:\n",
    "            failed_tasks.append({\n",
    "                'file_name': file_name,\n",
    "                'llm_output': generated_text,\n",
    "                'solution': gen_solution\n",
    "            })\n",
    "\n",
    "        # Store \"pixel accuracy for each task\n",
    "        accuracy_list.append({\n",
    "            'file_name': file_name,\n",
    "            'correct_percentage': correct_percentage\n",
    "        })\n",
    "\n",
    "    # Create a dictionary to store results\n",
    "    results = {'file_name': [], 'solved': [], 'accuracy': []}\n",
    "\n",
    "    # Add solved tasks to the results\n",
    "    for task in solved_tasks:\n",
    "        results['file_name'].append(task['file_name'])\n",
    "        results['solved'].append(True)\n",
    "        results['accuracy'].append(next((item['correct_percentage'] for item in accuracy_list if item['file_name'] == task['file_name']), None))\n",
    "\n",
    "    # Add failed tasks to the results\n",
    "    for task in failed_tasks:\n",
    "        results['file_name'].append(task['file_name'])\n",
    "        results['solved'].append(False)\n",
    "        results['accuracy'].append(next((item['correct_percentage'] for item in accuracy_list if item['file_name'] == task['file_name']), None))\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df_results = pd.DataFrame(results)\n",
    "\n",
    "    # Display the DataFrame as a table\n",
    "    print(df_results)\n",
    "\n",
    "    # Calculate and print the average correct percentage\n",
    "    average_correct_percentage = df_results['accuracy'].mean()\n",
    "    print(f\"Average 'Pixel Accuracy' of attempted tasks: {average_correct_percentage:.2f}%\")\n",
    "\n",
    "    # Calculate and print the number of solved tasks out of the total number of tasks\n",
    "    total_tasks = len(df)\n",
    "    solved_tasks_count = df_results['solved'].sum()\n",
    "    print(f\"Solved {solved_tasks_count} out of {total_tasks} tasks ({(solved_tasks_count / total_tasks) * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T06:18:15.684647Z",
     "iopub.status.busy": "2024-06-12T06:18:15.68437Z",
     "iopub.status.idle": "2024-06-12T06:18:15.710019Z",
     "shell.execute_reply": "2024-06-12T06:18:15.709221Z",
     "shell.execute_reply.started": "2024-06-12T06:18:15.684623Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "solution_dict = {}\n",
    "\n",
    "for i, task in enumerate(filtered_dataset):\n",
    "    file_name = task['file_name']\n",
    "    generated_text = task[\"generated_solution\"]\n",
    "    # Extract the solution generated by the model\n",
    "    gen_solution = extract_solution(generated_text)\n",
    "    # For now we only do one attempt\n",
    "    solution_dict[file_name] = [\n",
    "        {\n",
    "            \"attempt_1\": gen_solution,\n",
    "            \"attempt_2\": [[0, 0], [0, 0]]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "# Recombining the solutions for split files\n",
    "combined_solution_dict = {}\n",
    "combined_files = {}\n",
    "\n",
    "for file_name, attempts in solution_dict.items():\n",
    "    base_name = file_name.split('_')[0]\n",
    "    if base_name not in combined_solution_dict:\n",
    "        combined_solution_dict[base_name] = []\n",
    "        combined_files[base_name] = []\n",
    "    combined_solution_dict[base_name].extend(attempts)\n",
    "    if '_' in file_name:\n",
    "        combined_files[base_name].append(file_name)\n",
    "        \n",
    "# Printing which file names have been combined\n",
    "print(\"Files that have been combined:\")\n",
    "for base_name, files in combined_files.items():\n",
    "    if files:  # Print only if there are files that were combined\n",
    "        print(f\"{base_name}: {', '.join(files)}\")\n",
    "\n",
    "# We still need to fill in dummy solutions for the tasks we did not consider to make a valid submission:\n",
    "# Load the sample submission file\n",
    "with open('/kaggle/input/arc-prize-2024/sample_submission.json') as f:\n",
    "    sample_submission = json.load(f)\n",
    "# Fill in all entries that are still missing from the sample_submission file\n",
    "for key, value in sample_submission.items():\n",
    "    if key not in combined_solution_dict:\n",
    "        combined_solution_dict[key] = value\n",
    "\n",
    "# Create submission\n",
    "with open(\"submission.json\", \"w\") as json_file:\n",
    "    json.dump(combined_solution_dict, json_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8951125,
     "sourceId": 67357,
     "sourceType": "competition"
    },
    {
     "datasetId": 5123959,
     "sourceId": 8622192,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5174658,
     "sourceId": 8640486,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 39106,
     "modelInstanceId": 28083,
     "sourceId": 33551,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 121027,
     "modelInstanceId": 100931,
     "sourceId": 120000,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
