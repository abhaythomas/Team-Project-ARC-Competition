{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4abd8676",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-18T12:59:37.488807Z",
     "iopub.status.busy": "2024-11-18T12:59:37.488441Z",
     "iopub.status.idle": "2024-11-18T12:59:55.494195Z",
     "shell.execute_reply": "2024-11-18T12:59:55.493161Z"
    },
    "papermill": {
     "duration": 18.015046,
     "end_time": "2024-11-18T12:59:55.496791",
     "exception": false,
     "start_time": "2024-11-18T12:59:37.481745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /kaggle/input/llama-3-arc-deps\r\n",
      "Processing /kaggle/input/llama-3-arc-deps/trl-0.9.3-py3-none-any.whl (from -r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1))\r\n",
      "Processing /kaggle/input/llama-3-arc-deps/peft-0.11.1-py3-none-any.whl (from -r /kaggle/input/llama-3-arc-deps/requirements.txt (line 2))\r\n",
      "Processing /kaggle/input/llama-3-arc-deps/bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (from -r /kaggle/input/llama-3-arc-deps/requirements.txt (line 4))\r\n",
      "Requirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (2.4.0)\r\n",
      "Requirement already satisfied: transformers>=4.31.0 in /opt/conda/lib/python3.10/site-packages (from trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (4.45.1)\r\n",
      "Requirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.10/site-packages (from trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (1.26.4)\r\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (0.34.2)\r\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (3.0.1)\r\n",
      "Processing /kaggle/input/llama-3-arc-deps/tyro-0.8.4-py3-none-any.whl (from trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1))\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 2)) (21.3)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 2)) (5.9.3)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 2)) (6.0.2)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 2)) (4.66.4)\r\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 2)) (0.4.5)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 2)) (0.25.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 2)) (3.15.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 2)) (2024.6.1)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 2)) (2.32.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 2)) (4.12.2)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 2)) (3.1.2)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (3.1.4)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (2024.5.15)\r\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (0.20.0)\r\n",
      "Requirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (0.16)\r\n",
      "Requirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (13.7.1)\r\n",
      "Processing /kaggle/input/llama-3-arc-deps/shtab-1.7.1-py3-none-any.whl (from tyro>=0.5.11->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1))\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (16.1.0)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (2.2.2)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (0.70.16)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (3.9.5)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (23.2.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (1.4.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (6.0.5)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (1.9.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (4.0.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 2)) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 2)) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 2)) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 2)) (2024.8.30)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (2.18.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (2.1.5)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (2024.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (1.3.0)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (0.1.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl->-r /kaggle/input/llama-3-arc-deps/requirements.txt (line 1)) (1.16.0)\r\n",
      "Installing collected packages: shtab, tyro, bitsandbytes, trl, peft\r\n",
      "Successfully installed bitsandbytes-0.43.1 peft-0.11.1 shtab-1.7.1 trl-0.9.3 tyro-0.8.4\r\n"
     ]
    }
   ],
   "source": [
    "deps_path = '/kaggle/input/llama-3-arc-deps'\n",
    "! pip install --no-index --find-links {deps_path} --requirement {deps_path}/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e01c03b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T12:59:55.511528Z",
     "iopub.status.busy": "2024-11-18T12:59:55.511168Z",
     "iopub.status.idle": "2024-11-18T13:00:20.770989Z",
     "shell.execute_reply": "2024-11-18T13:00:20.770175Z"
    },
    "papermill": {
     "duration": 25.269796,
     "end_time": "2024-11-18T13:00:20.773455",
     "exception": false,
     "start_time": "2024-11-18T12:59:55.503659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import ast\n",
    "import re\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "# For LLM\n",
    "from peft import LoraConfig, PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    set_seed,\n",
    "    pipeline\n",
    ")\n",
    "from trl import setup_chat_format\n",
    "\n",
    "import torch\n",
    "from time import time\n",
    "\n",
    "# Set seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c846dcde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T13:00:20.788413Z",
     "iopub.status.busy": "2024-11-18T13:00:20.787392Z",
     "iopub.status.idle": "2024-11-18T13:02:22.701979Z",
     "shell.execute_reply": "2024-11-18T13:02:22.700885Z"
    },
    "papermill": {
     "duration": 121.923981,
     "end_time": "2024-11-18T13:02:22.704126",
     "exception": false,
     "start_time": "2024-11-18T13:00:20.780145",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c1217387d04528a1327c2145110be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare model, tokenizer: 121.904 sec.\n"
     ]
    }
   ],
   "source": [
    "# Define a template for formatting chat messages with the Llama 3 model\n",
    "# This is model specific. Change it if you e.g. use Google's Gemma instead of Llama\n",
    "#LLAMA_3_CHAT_TEMPLATE = \"\"\"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\"\"\"\n",
    "\n",
    "# Set the data type for computations to float16, bfloat16 not supported on T4/P100\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "# Configure the BitsAndBytes settings for 4-bit quantization to reduce memory usage\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Enable 4-bit quantization\n",
    "    bnb_4bit_use_double_quant=True,  # Use double quantization for improved precision\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Specify the quantization type\n",
    "    bnb_4bit_compute_dtype=compute_dtype,  # Set the computation data type\n",
    ")\n",
    "\n",
    "# Specify the model ID for loading the fine-tuned Llama 3 model\n",
    "# You can also test other models by replacing this line.\n",
    "# For the original non-finetuned model use\n",
    "# model_id = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"\n",
    "model_id = \"/kaggle/input/3.1-8b_instruct/transformers/default/1\"\n",
    "\n",
    "# Record the start time to measure the loading duratio\n",
    "time_start = time()\n",
    "print(\"Loading model\")\n",
    "# Load the pre-trained model with specified configurations\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True, # Allow the model to use custom code from the repository\n",
    "    quantization_config=bnb_config, # Apply the 4-bit quantization configuration\n",
    "    attn_implementation='sdpa', # Use scaled-dot product attention for better performance\n",
    "    torch_dtype=compute_dtype, # Set the data type for the model\n",
    "    use_cache=False, # Disable caching to save memory\n",
    "    device_map='auto', # Automatically map the model to available devices (e.g., GPUs)\n",
    ")\n",
    "\n",
    "# Load the tokenizer associated with the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "#tokenizer.chat_template = LLAMA_3_CHAT_TEMPLATE # Apply the chat message template\n",
    "\n",
    "# Record the end time and print the duration for preparing the model and tokenizer\n",
    "time_end = time()\n",
    "print(f\"Prepare model, tokenizer: {round(time_end-time_start, 3)} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c71632b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T13:02:22.718728Z",
     "iopub.status.busy": "2024-11-18T13:02:22.718398Z",
     "iopub.status.idle": "2024-11-18T13:02:22.859469Z",
     "shell.execute_reply": "2024-11-18T13:02:22.858656Z"
    },
    "papermill": {
     "duration": 0.15076,
     "end_time": "2024-11-18T13:02:22.861652",
     "exception": false,
     "start_time": "2024-11-18T13:02:22.710892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the files\n",
    "with open('/kaggle/input/arc-prize-2024/arc-agi_evaluation_challenges.json', 'r') as f:\n",
    "    challenges = json.load(f)\n",
    "\n",
    "with open('/kaggle/input/arc-prize-2024/arc-agi_evaluation_solutions.json', 'r') as f:\n",
    "    solutions = json.load(f)\n",
    "\n",
    "# Displaying the first few entries from challenges\n",
    "#print(\"Challenges - First few entries:\")\n",
    "#for idx, (key, value) in enumerate(challenges.items()):\n",
    "#    print(f\"Key: {key}, Value (truncated): {str(value)[:500]}...\")  # Adjust truncation as needed\n",
    "#    if idx >= 2:  # Limiting to first few examples for brevity\n",
    "#        break\n",
    "\n",
    "# Displaying the first few entries from solutions\n",
    "#print(\"\\nSolutions - First few entries:\")\n",
    "#for idx, (key, value) in enumerate(solutions.items()):\n",
    "#    print(f\"Key: {key}, Solution: {value}\")\n",
    "#    if idx >= 2:\n",
    "#        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a518380a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T13:02:22.877248Z",
     "iopub.status.busy": "2024-11-18T13:02:22.876686Z",
     "iopub.status.idle": "2024-11-18T13:02:42.081428Z",
     "shell.execute_reply": "2024-11-18T13:02:42.080400Z"
    },
    "papermill": {
     "duration": 19.215117,
     "end_time": "2024-11-18T13:02:42.083731",
     "exception": false,
     "start_time": "2024-11-18T13:02:22.868614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: 00576224\n",
      "Test Inputs: [[[3, 2], [7, 8]]]\n",
      "Model Outputs: ['Example Input: [[8, 6], [6, 4]]\\nExample Output: [[8, 6, 8, 6, 8, 6], [6, 4, 6, 4, 6, 4], [6, 8, 6, 8, 6, 8], [4, 6, 4, 6, 4, 6], [8, 6, 8, 6, 8, 6], [6, 4, 6, 4, 6, 4]]\\n\\nExample Input: [[7, 9], [4, 3]]\\nExample Output: [[7, 9, 7, 9, 7, 9], [4, 3, 4, 3, 4, 3], [9, 7, 9, 7, 9, 7], [3, 4, 3, 4, 3, 4], [7, 9, 7, 9, 7, 9], [4, 3, 4, 3, 4, 3]]\\n\\nGiven the previous examples, please apply the same transformation rule to the following input:\\nTest Input: [[3, 2], [7, 8]]\\nTransformation Result (Complete the full matrix): [[3, 2, 3, 2, 3, 2], [7, 8, 7, 8, 7, 8], [2, 3, 2, 3, 2, 3], [8, 7, 8, 7, 8, 7], [3, 2, 3, 2, 3, 2], [7, 8, 7, 8, 7, 8]]\\nSolution to input1:\\n        [[3, 2, 3, 2, 3, 2], [7, 8, 7, 8, 7, 8], [2, 3, 2, 3, 2, 3], [8, 7, 8, 7, 8, 7], [3, 2, 3, 2, 3, 2], [7, 8, 7, 8, 7, 8]]\\n        ']\n",
      "Expected Solution: [[[3, 2, 3, 2, 3, 2], [7, 8, 7, 8, 7, 8], [2, 3, 2, 3, 2, 3], [8, 7, 8, 7, 8, 7], [3, 2, 3, 2, 3, 2], [7, 8, 7, 8, 7, 8]]]\n",
      "Result: No Match\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def evaluate_single_example_with_reasoning(model, tokenizer, challenges, solutions, key):\n",
    "    # Determine the device the model is already on\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Retrieve the specified challenge data\n",
    "    if key not in challenges:\n",
    "        print(f\"Key {key} not found in challenges.\")\n",
    "        return\n",
    "\n",
    "    challenge_data = challenges[key]\n",
    "    test_inputs = [test_case['input'] for test_case in challenge_data['test']]\n",
    "    training_examples = challenge_data['train']\n",
    "\n",
    "    # Generate model output for each test input, while including previous examples for reasoning\n",
    "    model_outputs = []\n",
    "    for input_data in test_inputs:\n",
    "        # Format the training examples into a chain of thought\n",
    "        examples = \"\"\n",
    "        for example in training_examples:\n",
    "            input_example = example['input']\n",
    "            output_example = example['output']\n",
    "            # Format the example input and output with reasoning prompt\n",
    "            examples += f\"Example Input: {input_example}\\nExample Output: {output_example}\\n\\n\"\n",
    "        \n",
    "        # Add the test input with an instruction for the model to apply the same transformation\n",
    "        input_text = f\"Given the previous examples, please apply the same transformation rule to the following input:\\nTest Input: {input_data}\\nTransformation Result (Complete the full matrix):\"\n",
    "\n",
    "        # Combine the training examples and the test input into one prompt\n",
    "        full_prompt = examples + input_text\n",
    "\n",
    "        # Tokenize the prompt and move it to the model's device\n",
    "        inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # Remove 'token_type_ids' if present, as LLaMA models do not use it\n",
    "        if 'token_type_ids' in inputs:\n",
    "            del inputs['token_type_ids']\n",
    "\n",
    "        # Generate model output with more tokens to ensure full matrix is generated\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                **inputs, \n",
    "                max_new_tokens=300,  # Increased limit to allow for a larger output\n",
    "                temperature=0.7,    # Control randomness\n",
    "                top_p=0.9,          # Nucleus sampling\n",
    "                do_sample=False     # Use greedy decoding to reduce randomness\n",
    "            )\n",
    "        output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        model_outputs.append(output_text)\n",
    "    \n",
    "    # Retrieve the expected solution\n",
    "    expected_solution = solutions.get(key)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Key: {key}\")\n",
    "    print(f\"Test Inputs: {test_inputs}\")\n",
    "    print(f\"Model Outputs: {model_outputs}\")\n",
    "    print(f\"Expected Solution: {expected_solution}\")\n",
    "\n",
    "    # Compare model output with expected solution (adjust comparison logic as needed)\n",
    "    if model_outputs == expected_solution:\n",
    "        print(\"Result: Match\")\n",
    "    else:\n",
    "        print(\"Result: No Match\")\n",
    "\n",
    "# Example usage with a specific key (replace '00576224' with your desired key)\n",
    "evaluate_single_example_with_reasoning(model, tokenizer, challenges, solutions, '00576224')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ce4719f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T13:02:42.099231Z",
     "iopub.status.busy": "2024-11-18T13:02:42.098894Z",
     "iopub.status.idle": "2024-11-18T13:02:42.113264Z",
     "shell.execute_reply": "2024-11-18T13:02:42.112533Z"
    },
    "papermill": {
     "duration": 0.024191,
     "end_time": "2024-11-18T13:02:42.115099",
     "exception": false,
     "start_time": "2024-11-18T13:02:42.090908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def extract_matrix_from_output(output_text):\n",
    "    # This function extracts the matrix from the model's output by cutting off unwanted parts.\n",
    "    \n",
    "    # Find the portion of the output after \"Solution to input1:\" and clean it up.\n",
    "    if \"Solution to input1:\" in output_text:\n",
    "        matrix_text = output_text.split(\"Solution to input1:\")[-1]  # Get the part after \"Solution to input1:\"\n",
    "    else:\n",
    "        matrix_text = output_text  # If the string isn't found, use the full output\n",
    "    \n",
    "    # Clean up the string by removing extra spaces and newlines\n",
    "    matrix_text = matrix_text.strip()\n",
    "    matrix_text = matrix_text.replace(\"\\n\", \" \")  # Replace line breaks with spaces\n",
    "    matrix_text = matrix_text.replace(\"        \", \"\")  # Remove excess spaces (indentation)\n",
    "    \n",
    "    # Try to convert the cleaned-up string to a list (matrix form)\n",
    "    try:\n",
    "        matrix = eval(matrix_text)  # Convert the string representation of a list to a Python list\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing matrix: {e}\")\n",
    "        matrix = None\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "def evaluate_single_example_with_reasoning(model, tokenizer, challenges, solutions, key):\n",
    "    # Determine the device the model is already on\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Retrieve the specified challenge data\n",
    "    if key not in challenges:\n",
    "        print(f\"Key {key} not found in challenges.\")\n",
    "        return\n",
    "\n",
    "    challenge_data = challenges[key]\n",
    "    test_inputs = [test_case['input'] for test_case in challenge_data['test']]\n",
    "    training_examples = challenge_data['train']\n",
    "\n",
    "    # Generate model output for each test input, while including previous examples for reasoning\n",
    "    model_outputs = []\n",
    "    for input_data in test_inputs:\n",
    "        # Format the training examples into a chain of thought\n",
    "        examples = \"\"\n",
    "        for example in training_examples:\n",
    "            input_example = example['input']\n",
    "            output_example = example['output']\n",
    "            # Format the example input and output with reasoning prompt\n",
    "            examples += f\"Example Input: {input_example}\\nExample Output: {output_example}\\n\\n\"\n",
    "        \n",
    "        # Add the test input with an instruction for the model to apply the same transformation\n",
    "        input_text = f\"Given the previous examples, please apply the same transformation rule to the following input:\\nTest Input: {input_data}\\nTransformation Result (Complete the full matrix):\"\n",
    "\n",
    "        # Combine the training examples and the test input into one prompt\n",
    "        full_prompt = examples + input_text\n",
    "\n",
    "        # Tokenize the prompt and move it to the model's device\n",
    "        inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # Remove 'token_type_ids' if present, as LLaMA models do not use it\n",
    "        if 'token_type_ids' in inputs:\n",
    "            del inputs['token_type_ids']\n",
    "\n",
    "        # Generate model output with more tokens to ensure full matrix is generated\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                **inputs, \n",
    "                #max_new_tokens=300,  # Increased limit to allow for a larger output\n",
    "                temperature=0.7,    # Control randomness\n",
    "                top_p=0.9,          # Nucleus sampling\n",
    "                do_sample=False     # Use greedy decoding to reduce randomness\n",
    "            )\n",
    "        output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        # Extract the matrix from the model's output\n",
    "        extracted_matrix = extract_matrix_from_output(output_text)\n",
    "        \n",
    "        if extracted_matrix is not None:\n",
    "            model_outputs.append(extracted_matrix)\n",
    "    \n",
    "    # Retrieve the expected solution\n",
    "    expected_solution = solutions.get(key)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Key: {key}\")\n",
    "    print(f\"Test Inputs: {test_inputs}\")\n",
    "    print(f\"Model Outputs: {model_outputs}\")\n",
    "    print(f\"Expected Solution: {expected_solution}\")\n",
    "\n",
    "    # Compare the extracted model output with the expected solution (matrix only)\n",
    "    if model_outputs == expected_solution:\n",
    "        print(\"Result: Match\")\n",
    "    else:\n",
    "        print(\"Result: No Match\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5db124eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T13:02:42.130411Z",
     "iopub.status.busy": "2024-11-18T13:02:42.130095Z",
     "iopub.status.idle": "2024-11-18T13:03:00.535117Z",
     "shell.execute_reply": "2024-11-18T13:03:00.534228Z"
    },
    "papermill": {
     "duration": 18.415054,
     "end_time": "2024-11-18T13:03:00.537462",
     "exception": false,
     "start_time": "2024-11-18T13:02:42.122408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: 00576224\n",
      "Test Inputs: [[[3, 2], [7, 8]]]\n",
      "Model Outputs: [[[3, 2, 3, 2, 3, 2], [7, 8, 7, 8, 7, 8], [2, 3, 2, 3, 2, 3], [8, 7, 8, 7, 8, 7], [3, 2, 3, 2, 3, 2], [7, 8, 7, 8, 7, 8]]]\n",
      "Expected Solution: [[[3, 2, 3, 2, 3, 2], [7, 8, 7, 8, 7, 8], [2, 3, 2, 3, 2, 3], [8, 7, 8, 7, 8, 7], [3, 2, 3, 2, 3, 2], [7, 8, 7, 8, 7, 8]]]\n",
      "Result: Match\n"
     ]
    }
   ],
   "source": [
    "evaluate_single_example_with_reasoning(model, tokenizer, challenges, solutions, '00576224')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "574e10d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T13:03:00.553502Z",
     "iopub.status.busy": "2024-11-18T13:03:00.553167Z",
     "iopub.status.idle": "2024-11-18T13:03:00.665498Z",
     "shell.execute_reply": "2024-11-18T13:03:00.664094Z"
    },
    "papermill": {
     "duration": 0.122728,
     "end_time": "2024-11-18T13:03:00.667643",
     "exception": false,
     "start_time": "2024-11-18T13:03:00.544915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d56f2372\n",
      "212895b5\n",
      "0a2355a6\n",
      "f21745ec\n",
      "5af49b42\n",
      "516b51b7\n",
      "4c177718\n",
      "2a5f8217\n",
      "f0afb749\n",
      "1e97544e\n",
      "e41c6fd3\n",
      "b942fd60\n",
      "1acc24af\n",
      "c7d4e6ad\n",
      "903d1b4a\n",
      "0c786b71\n",
      "0becf7df\n",
      "1c56ad9f\n",
      "4aab4007\n",
      "4ff4c9da\n",
      "aab50785\n",
      "ca8f78db\n",
      "0b17323b\n",
      "bd14c3bf\n",
      "42a15761\n",
      "e99362f0\n",
      "da515329\n",
      "e78887d1\n",
      "8fbca751\n",
      "4acc7107\n",
      "96a8c0cd\n",
      "c6e1b8da\n",
      "5b692c0f\n",
      "03560426\n",
      "f83cb3f6\n",
      "32e9702f\n",
      "e760a62e\n",
      "72207abc\n",
      "31adaf00\n",
      "48f8583b\n",
      "f9a67cb5\n",
      "705a3229\n",
      "817e6c09\n",
      "1d398264\n",
      "79369cc6\n",
      "73182012\n",
      "cad67732\n",
      "5833af48\n",
      "11e1fe23\n",
      "ecaa0ec1\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "# Load the challenge dataset\n",
    "with open(\"/kaggle/input/arc-prize-2024/arc-agi_evaluation_challenges.json\", \"r\") as f:\n",
    "    examples = json.load(f)\n",
    "\n",
    "# Randomly select 50 keys from the dataset\n",
    "random_keys = random.sample(list(examples.keys()), 50)\n",
    "\n",
    "# Print the selected keys\n",
    "for key in random_keys:\n",
    "    print(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb0274c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T13:03:00.683618Z",
     "iopub.status.busy": "2024-11-18T13:03:00.683292Z",
     "iopub.status.idle": "2024-11-18T13:03:19.165644Z",
     "shell.execute_reply": "2024-11-18T13:03:19.164660Z"
    },
    "papermill": {
     "duration": 18.492951,
     "end_time": "2024-11-18T13:03:19.168094",
     "exception": false,
     "start_time": "2024-11-18T13:03:00.675143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating key: 00576224...\n",
      "Generating output...\n",
      "Decoded output text: Example Input: [[8, 6], [6, 4]]\n",
      "Example Output: [[8, 6, 8, 6, 8, 6], [6, 4, 6, 4, 6, 4], [6, 8, 6, 8, 6, 8], [4, 6, 4, 6, 4, 6], [8, 6, 8, 6, 8, 6], [6, 4, 6, 4, 6, 4]]\n",
      "\n",
      "Example Input: [[7, 9], [4, 3]]\n",
      "Example Output: [[7, 9, 7, 9, 7, 9], [4, 3, 4, 3, 4, 3], [9, 7, 9, 7, 9, 7], [3, 4, 3, 4, 3, 4], [7, 9, 7, 9, 7, 9], [4, 3, 4, 3, 4, 3]]\n",
      "\n",
      "Given the previous examples, please apply the same transformation rule to the following input:\n",
      "Test Input: [[3, 2], [7, 8]]\n",
      "Transformation Result (Complete the full matrix): [[3, 2, 3, 2, 3, 2], [7, 8, 7, 8, 7, 8], [2, 3, 2, 3, 2, 3], [8, 7, 8, 7, 8, 7], [3, 2, 3, 2, 3, 2], [7, 8, 7, 8, 7, 8]]\n",
      "Solution to input1:\n",
      "        [[3, 2, 3, 2, 3, 2], [7, 8, 7, 8, 7, 8], [2, 3, 2, 3, 2, 3], [8, 7, 8, 7, 8, 7], [3, 2, 3, 2, 3, 2], [7, 8, 7, 8, 7, 8]]\n",
      "        \n",
      "Key: 00576224\n",
      "Test Inputs: [[[3, 2], [7, 8]]]\n",
      "Model Outputs: Example Input: [[8, 6], [6, 4]]\n",
      "Example Output: [[8, 6, 8, 6, 8, 6], [6, 4, 6, 4, 6, 4], [6, 8, 6, 8, 6, 8], [4, 6, 4, 6, 4, 6], [8, 6, 8, 6, 8, 6], [6, 4, 6, 4, 6, 4]]\n",
      "\n",
      "Example Input: [[7, 9], [4, 3]]\n",
      "Example Output: [[7, 9, 7, 9, 7, 9], [4, 3, 4, 3, 4, 3], [9, 7, 9, 7, 9, 7], [3, 4, 3, 4, 3, 4], [7, 9, 7, 9, 7, 9], [4, 3, 4, 3, 4, 3]]\n",
      "\n",
      "Given the previous examples, please apply the same transformation rule to the following input:\n",
      "Test Input: [[3, 2], [7, 8]]\n",
      "Transformation Result (Complete the full matrix): [[3, 2, 3, 2, 3, 2], [7, 8, 7, 8, 7, 8], [2, 3, 2, 3, 2, 3], [8, 7, 8, 7, 8, 7], [3, 2, 3, 2, 3, 2], [7, 8, 7, 8, 7, 8]]\n",
      "Solution to input1:\n",
      "        [[3, 2, 3, 2, 3, 2], [7, 8, 7, 8, 7, 8], [2, 3, 2, 3, 2, 3], [8, 7, 8, 7, 8, 7], [3, 2, 3, 2, 3, 2], [7, 8, 7, 8, 7, 8]]\n",
      "        \n",
      "Expected Solution: [[[3, 2, 3, 2, 3, 2], [7, 8, 7, 8, 7, 8], [2, 3, 2, 3, 2, 3], [8, 7, 8, 7, 8, 7], [3, 2, 3, 2, 3, 2], [7, 8, 7, 8, 7, 8]]]\n",
      "Result: Match\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def extract_matrix_from_output(output_text):\n",
    "    # Extracts the matrix from the model's output by cutting off unwanted parts.\n",
    "    if \"Solution to input1:\" in output_text:\n",
    "        matrix_text = output_text.split(\"Solution to input1:\")[-1]\n",
    "    else:\n",
    "        matrix_text = output_text\n",
    "    matrix_text = matrix_text.strip().replace(\"\\n\", \" \").replace(\"        \", \"\")\n",
    "    try:\n",
    "        import ast\n",
    "        matrix = ast.literal_eval(matrix_text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing matrix: {e}\")\n",
    "        matrix = None\n",
    "    return matrix\n",
    "\n",
    "def evaluate_single_example_with_reasoning(model, tokenizer, challenges, solutions, key):\n",
    "    print(f\"Evaluating key: {key}...\")\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    if key not in challenges:\n",
    "        print(f\"Key {key} not found in challenges.\")\n",
    "        return\n",
    "    \n",
    "    challenge_data = challenges[key]\n",
    "    test_inputs = [test_case['input'] for test_case in challenge_data['test']]\n",
    "    training_examples = challenge_data['train']\n",
    "    \n",
    "    model_outputs = []\n",
    "    for input_data in test_inputs:\n",
    "        examples = \"\"\n",
    "        for example in training_examples:\n",
    "            input_example = example['input']\n",
    "            output_example = example['output']\n",
    "            examples += f\"Example Input: {input_example}\\nExample Output: {output_example}\\n\\n\"\n",
    "        \n",
    "        input_text = f\"Given the previous examples, please apply the same transformation rule to the following input:\\nTest Input: {input_data}\\nTransformation Result (Complete the full matrix):\"\n",
    "        full_prompt = examples + input_text\n",
    "        \n",
    "        # Tokenize input with a higher max_length setting\n",
    "        max_input_length = 2048  # Match with fine-tuning context length\n",
    "        inputs = tokenizer(full_prompt, return_tensors=\"pt\", truncation=True, max_length=max_input_length).to(device)\n",
    "        \n",
    "        if 'token_type_ids' in inputs:\n",
    "            del inputs['token_type_ids']\n",
    "        \n",
    "        print(\"Generating output...\")\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                output_ids = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=2048,  # Adjust this value if needed\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.9,\n",
    "                    do_sample=False\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error during generation: {e}\")\n",
    "                continue  # Skip to the next test input if generation fails\n",
    "        \n",
    "        output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        print(f\"Decoded output text: {output_text}\")\n",
    "        \n",
    "        extracted_matrix = extract_matrix_from_output(output_text)\n",
    "        \n",
    "        if extracted_matrix is not None:\n",
    "            model_outputs.append(extracted_matrix)\n",
    "    \n",
    "    expected_solution = solutions.get(key)\n",
    "    \n",
    "    print(f\"Key: {key}\")\n",
    "    print(f\"Test Inputs: {test_inputs}\")\n",
    "    print(f\"Model Outputs: {output_text}\")\n",
    "    print(f\"Expected Solution: {expected_solution}\")\n",
    "\n",
    "    if model_outputs == expected_solution:\n",
    "        print(\"Result: Match\")\n",
    "    else:\n",
    "        print(\"Result: No Match\")\n",
    "\n",
    "# Example usage with a specific key\n",
    "evaluate_single_example_with_reasoning(model, tokenizer, challenges, solutions, '00576224')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9c2b813",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T13:03:19.184187Z",
     "iopub.status.busy": "2024-11-18T13:03:19.183893Z",
     "iopub.status.idle": "2024-11-18T13:06:09.425487Z",
     "shell.execute_reply": "2024-11-18T13:06:09.424522Z"
    },
    "papermill": {
     "duration": 170.260084,
     "end_time": "2024-11-18T13:06:09.435739",
     "exception": false,
     "start_time": "2024-11-18T13:03:19.175655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating key: e0fb7511...\n",
      "Generating output...\n",
      "Decoded output text: Example Input: [[1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1], [1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0], [1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1], [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1], [0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1], [1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1], [0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1]]\n",
      "Example Output: [[1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1], [1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 8, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 8, 8, 8, 1, 1, 8, 1, 1, 0], [1, 1, 8, 8, 1, 1, 8, 1, 1, 8, 1, 1, 1], [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1], [0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1], [1, 0, 1, 1, 1, 1, 8, 8, 1, 1, 1, 1, 1], [0, 1, 1, 1, 1, 1, 1, 8, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 8, 8, 1, 1]]\n",
      "\n",
      "Example Input: [[1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1], [1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1], [1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], [1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1], [1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1], [1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1], [1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1], [1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0], [1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1], [0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1], [0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1], [1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1], [1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1]]\n",
      "Example Output: [[1, 1, 1, 8, 8, 1, 1, 1, 8, 1, 0, 1, 1], [1, 1, 0, 1, 1, 1, 1, 1, 8, 8, 1, 0, 1], [1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], [1, 1, 8, 8, 1, 0, 1, 1, 0, 1, 1, 1, 1], [1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1], [1, 1, 1, 0, 1, 1, 1, 0, 1, 8, 1, 1, 1], [1, 8, 8, 1, 1, 1, 0, 1, 1, 8, 8, 1, 1], [1, 8, 8, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0], [1, 8, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1], [8, 8, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1], [8, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1], [1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 8, 1], [1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 8, 8, 1]]\n",
      "\n",
      "Example Input: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "Error parsing matrix: invalid syntax (<unknown>, line 1)\n",
      "Key: e0fb7511\n",
      "Test Inputs: [[[1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1], [0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0], [0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1], [1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1], [1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0], [1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0], [1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0], [1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0], [1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1], [0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1], [1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0], [1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1]]]\n",
      "Model Outputs: Example Input: [[1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1], [1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0], [1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1], [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1], [0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1], [1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1], [0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1]]\n",
      "Example Output: [[1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1], [1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 8, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 8, 8, 8, 1, 1, 8, 1, 1, 0], [1, 1, 8, 8, 1, 1, 8, 1, 1, 8, 1, 1, 1], [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1], [0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1], [1, 0, 1, 1, 1, 1, 8, 8, 1, 1, 1, 1, 1], [0, 1, 1, 1, 1, 1, 1, 8, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 8, 8, 1, 1]]\n",
      "\n",
      "Example Input: [[1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1], [1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1], [1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], [1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1], [1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1], [1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1], [1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1], [1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0], [1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1], [0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1], [0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1], [1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1], [1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1]]\n",
      "Example Output: [[1, 1, 1, 8, 8, 1, 1, 1, 8, 1, 0, 1, 1], [1, 1, 0, 1, 1, 1, 1, 1, 8, 8, 1, 0, 1], [1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], [1, 1, 8, 8, 1, 0, 1, 1, 0, 1, 1, 1, 1], [1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1], [1, 1, 1, 0, 1, 1, 1, 0, 1, 8, 1, 1, 1], [1, 8, 8, 1, 1, 1, 0, 1, 1, 8, 8, 1, 1], [1, 8, 8, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0], [1, 8, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1], [8, 8, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1], [8, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1], [1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 8, 1], [1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 8, 8, 1]]\n",
      "\n",
      "Example Input: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "Expected Solution: [[[1, 1, 1, 8, 8, 1, 1, 1, 1, 1, 8, 8, 1], [0, 1, 1, 1, 8, 1, 1, 1, 1, 1, 1, 8, 8], [1, 1, 8, 8, 8, 1, 1, 0, 1, 1, 0, 1, 8], [0, 1, 1, 8, 1, 1, 1, 1, 1, 0, 1, 0, 1], [1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1], [1, 8, 1, 0, 1, 1, 8, 1, 1, 1, 0, 1, 8], [1, 8, 8, 1, 0, 1, 8, 1, 1, 1, 1, 1, 8], [1, 8, 1, 0, 1, 1, 1, 1, 8, 8, 8, 1, 8], [1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 8, 1, 8], [1, 0, 1, 8, 8, 1, 1, 1, 1, 8, 8, 1, 1], [0, 1, 1, 8, 1, 1, 1, 1, 0, 1, 8, 1, 1], [1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 8, 1, 0], [1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1]]]\n",
      "Result: No Match\n"
     ]
    }
   ],
   "source": [
    "evaluate_single_example_with_reasoning(model, tokenizer, challenges, solutions, 'e0fb7511')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8dbe533",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T13:06:09.451889Z",
     "iopub.status.busy": "2024-11-18T13:06:09.451315Z",
     "iopub.status.idle": "2024-11-18T13:06:09.456022Z",
     "shell.execute_reply": "2024-11-18T13:06:09.455118Z"
    },
    "papermill": {
     "duration": 0.014957,
     "end_time": "2024-11-18T13:06:09.458078",
     "exception": false,
     "start_time": "2024-11-18T13:06:09.443121",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1d7f3d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T13:06:09.474113Z",
     "iopub.status.busy": "2024-11-18T13:06:09.473829Z",
     "iopub.status.idle": "2024-11-18T13:09:00.806977Z",
     "shell.execute_reply": "2024-11-18T13:09:00.806045Z"
    },
    "papermill": {
     "duration": 171.352482,
     "end_time": "2024-11-18T13:09:00.817930",
     "exception": false,
     "start_time": "2024-11-18T13:06:09.465448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating key: 414297c0...\n",
      "Generating output...\n",
      "Decoded output text: Example Input: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 2, 0, 0, 0, 2, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 2, 7, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [0, 0, 0, 0, 2, 4, 2, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [0, 2, 8, 2, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 0], [0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 8, 1, 1, 1, 1, 7, 1, 1, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "Example Output: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 2, 3, 2, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1], [1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1], [1, 1, 1, 2, 4, 2, 1, 1, 1, 1, 1], [1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 2, 2, 2, 1, 1, 1, 2, 1, 1, 1], [1, 2, 8, 2, 1, 1, 2, 7, 2, 1, 1], [1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "\n",
      "Example Input: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0,\n",
      "Error parsing matrix: invalid syntax (<unknown>, line 1)\n",
      "Key: 414297c0\n",
      "Test Inputs: [[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 2, 7, 2, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 2, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 2, 2, 0, 0, 0], [0, 4, 4, 4, 1, 4, 4, 4, 4, 4, 4, 4, 4, 7, 4, 4, 0, 0, 0, 0, 8, 2, 0, 0], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 2, 2, 0, 0], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 6, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0], [0, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 8, 4, 0, 0, 0, 0, 0, 0, 0, 0], [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 2, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]\n",
      "Model Outputs: Example Input: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 2, 0, 0, 0, 2, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 2, 7, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [0, 0, 0, 0, 2, 4, 2, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [0, 2, 8, 2, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 0], [0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 8, 1, 1, 1, 1, 7, 1, 1, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "Example Output: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 2, 3, 2, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1], [1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1], [1, 1, 1, 2, 4, 2, 1, 1, 1, 1, 1], [1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 2, 2, 2, 1, 1, 1, 2, 1, 1, 1], [1, 2, 8, 2, 1, 1, 2, 7, 2, 1, 1], [1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "\n",
      "Example Input: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0], [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0,\n",
      "Expected Solution: [[[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], [4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], [4, 4, 2, 1, 2, 4, 4, 4, 4, 4, 4, 2, 7, 2, 4], [4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4], [4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 4, 4, 4], [4, 4, 4, 4, 4, 4, 4, 4, 2, 6, 4, 4, 4, 4, 4], [4, 2, 2, 4, 4, 4, 4, 4, 4, 4, 2, 4, 2, 2, 4], [2, 3, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 8, 2], [2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2]]]\n",
      "Result: No Match\n"
     ]
    }
   ],
   "source": [
    "evaluate_single_example_with_reasoning(model, tokenizer, challenges, solutions, '414297c0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9af007f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T13:09:00.834752Z",
     "iopub.status.busy": "2024-11-18T13:09:00.834425Z",
     "iopub.status.idle": "2024-11-18T13:09:00.884184Z",
     "shell.execute_reply": "2024-11-18T13:09:00.883274Z"
    },
    "papermill": {
     "duration": 0.06043,
     "end_time": "2024-11-18T13:09:00.886213",
     "exception": false,
     "start_time": "2024-11-18T13:09:00.825783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': [{'input': [[0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0],\n",
       "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 0, 2, 7, 2, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 2, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 0, 0, 0, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 2, 2, 0, 0, 0],\n",
       "    [0, 4, 4, 4, 1, 4, 4, 4, 4, 4, 4, 4, 4, 7, 4, 4, 0, 0, 0, 0, 8, 2, 0, 0],\n",
       "    [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 2, 2, 0, 0],\n",
       "    [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 6, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 8, 4, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 0, 0, 0, 0, 0, 2, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0]]}],\n",
       " 'train': [{'input': [[0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0],\n",
       "    [0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 0, 2, 0, 0, 0, 2, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 2, 7, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "    [0, 0, 0, 0, 2, 4, 2, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "    [0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 0],\n",
       "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "    [0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "    [0, 2, 8, 2, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 0],\n",
       "    [0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 8, 1, 1, 1, 1, 7, 1, 1, 1, 0],\n",
       "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "   'output': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "    [1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1],\n",
       "    [1, 1, 1, 1, 1, 1, 1, 2, 3, 2, 1],\n",
       "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1],\n",
       "    [1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1],\n",
       "    [1, 1, 1, 2, 4, 2, 1, 1, 1, 1, 1],\n",
       "    [1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1],\n",
       "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "    [1, 2, 2, 2, 1, 1, 1, 2, 1, 1, 1],\n",
       "    [1, 2, 8, 2, 1, 1, 2, 7, 2, 1, 1],\n",
       "    [1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1],\n",
       "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]},\n",
       "  {'input': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 2, 0],\n",
       "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 2, 0],\n",
       "    [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0],\n",
       "    [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 8, 1, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 4, 2, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 8, 8, 8, 4, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "   'output': [[8, 8, 8, 8, 8, 8],\n",
       "    [8, 2, 8, 8, 8, 8],\n",
       "    [2, 1, 2, 8, 8, 8],\n",
       "    [2, 8, 8, 8, 8, 8],\n",
       "    [8, 8, 8, 8, 8, 8],\n",
       "    [8, 8, 8, 8, 8, 8],\n",
       "    [8, 8, 2, 8, 2, 8],\n",
       "    [8, 8, 8, 4, 2, 8],\n",
       "    [8, 8, 2, 2, 2, 8]]},\n",
       "  {'input': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0],\n",
       "    [0, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0],\n",
       "    [0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0],\n",
       "    [0, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0],\n",
       "    [0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 8, 3, 3, 0, 0, 0, 0, 0],\n",
       "    [0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0],\n",
       "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0],\n",
       "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0],\n",
       "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0],\n",
       "    [0, 0, 0, 2, 2, 0, 0, 0, 0, 2, 1, 2, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 0, 0, 2, 8, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "   'output': [[3, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3],\n",
       "    [3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3],\n",
       "    [3, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3],\n",
       "    [3, 2, 1, 2, 3, 3, 3, 3, 2, 2, 3, 3],\n",
       "    [3, 3, 2, 3, 3, 3, 3, 3, 2, 8, 3, 3],\n",
       "    [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3]]}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "challenges['414297c0']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2e76c66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T13:09:00.903803Z",
     "iopub.status.busy": "2024-11-18T13:09:00.903521Z",
     "iopub.status.idle": "2024-11-18T13:09:00.915573Z",
     "shell.execute_reply": "2024-11-18T13:09:00.914811Z"
    },
    "papermill": {
     "duration": 0.02293,
     "end_time": "2024-11-18T13:09:00.917414",
     "exception": false,
     "start_time": "2024-11-18T13:09:00.894484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_single_example_with_reasoning(model, tokenizer, challenges, solutions, key):\n",
    "    print(f\"Evaluating key: {key}...\")\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    if key not in challenges:\n",
    "        print(f\"Key {key} not found in challenges.\")\n",
    "        return\n",
    "    \n",
    "    challenge_data = challenges[key]\n",
    "    test_inputs = [test_case['input'] for test_case in challenge_data['test']]\n",
    "    training_examples = challenge_data['train']\n",
    "    \n",
    "    model_outputs = []\n",
    "    for input_data in test_inputs:\n",
    "        examples = \"\"\n",
    "        for example in training_examples:\n",
    "            input_example = example['input']\n",
    "            output_example = example['output']\n",
    "            examples += f\"Example Input: {input_example}\\nExample Output: {output_example}\\n\\n\"\n",
    "        \n",
    "        input_text = f\"Given the previous examples, please apply the same transformation rule to the following input:\\nTest Input: {input_data}\\nTransformation Result (Complete the full matrix):\"\n",
    "        full_prompt = examples + input_text\n",
    "        \n",
    "        # Tokenization length check before generating output\n",
    "        inputs = tokenize_and_check_length(full_prompt, tokenizer)\n",
    "        if inputs is None:\n",
    "            print(\"Skipping input due to excessive token length.\")\n",
    "            continue\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        if 'token_type_ids' in inputs:\n",
    "            del inputs['token_type_ids']\n",
    "        \n",
    "        print(\"Generating output...\")\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                output_ids = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=2048,  # Adjust this value if needed\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.9,\n",
    "                    do_sample=False\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error during generation: {e}\")\n",
    "                continue  # Skip to the next test input if generation fails\n",
    "        \n",
    "        output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        print(f\"Decoded output text: {output_text}\")\n",
    "        \n",
    "        extracted_matrix = extract_matrix_from_output(output_text)\n",
    "        \n",
    "        if extracted_matrix is not None:\n",
    "            model_outputs.append(extracted_matrix)\n",
    "    \n",
    "    expected_solution = solutions.get(key)\n",
    "    \n",
    "    print(f\"Key: {key}\")\n",
    "    print(f\"Test Inputs: {test_inputs}\")\n",
    "    print(f\"Model Outputs: {model_outputs}\")\n",
    "    print(f\"Expected Solution: {expected_solution}\")\n",
    "\n",
    "    if model_outputs == expected_solution:\n",
    "        print(\"Result: Match\")\n",
    "    else:\n",
    "        print(\"Result: No Match\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8951125,
     "sourceId": 67357,
     "sourceType": "competition"
    },
    {
     "datasetId": 5123959,
     "sourceId": 8622192,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 165284,
     "modelInstanceId": 142702,
     "sourceId": 167742,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 570.040706,
   "end_time": "2024-11-18T13:09:04.475784",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-18T12:59:34.435078",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "02c1217387d04528a1327c2145110be3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e104db378ec04cde97b8c21a987b2d0c",
        "IPY_MODEL_42fdc25004a14992b4e344354855cefb",
        "IPY_MODEL_2ddb4b6902e7463c9f0727505d903102"
       ],
       "layout": "IPY_MODEL_9755da69501f420b9a30f2a431f5293d"
      }
     },
     "2ddb4b6902e7463c9f0727505d903102": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c930eee1a44643b39504901cce379ea8",
       "placeholder": "​",
       "style": "IPY_MODEL_644345e78e714eb7b1e13857ac75d92e",
       "value": " 4/4 [01:59&lt;00:00, 25.81s/it]"
      }
     },
     "42fdc25004a14992b4e344354855cefb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6878834370f6497eab4f419d1a47d01a",
       "max": 4.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_66a5a13da6dd4d45b2bf75da2387bf32",
       "value": 4.0
      }
     },
     "644345e78e714eb7b1e13857ac75d92e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "66a5a13da6dd4d45b2bf75da2387bf32": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "6878834370f6497eab4f419d1a47d01a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "68a92cf413b740ae8ee80c25385dd939": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "9755da69501f420b9a30f2a431f5293d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c930eee1a44643b39504901cce379ea8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "deb78a7848c347ab995edb4a0401525a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e104db378ec04cde97b8c21a987b2d0c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_deb78a7848c347ab995edb4a0401525a",
       "placeholder": "​",
       "style": "IPY_MODEL_68a92cf413b740ae8ee80c25385dd939",
       "value": "Loading checkpoint shards: 100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
